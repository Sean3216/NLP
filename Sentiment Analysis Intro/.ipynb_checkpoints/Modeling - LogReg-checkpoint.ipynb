{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db93df23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#essentials\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0e77338",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect('database.sqlite')\n",
    "raw = pd.read_sql_query(\"select ProductId,ProfileName, HelpfulnessNumerator, HelpfulnessDenominator, Time, Text, case Score when 5 then 1 when 4 then 0 else -1 end Sentiment from Reviews\", con)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a86d63cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProductId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Time</th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568449</th>\n",
       "      <td>B001EO7N10</td>\n",
       "      <td>Lettie D. Carter</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1299628800</td>\n",
       "      <td>Great for sesame chicken..this is a good if no...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568450</th>\n",
       "      <td>B003S1WTCU</td>\n",
       "      <td>R. Sawyer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1331251200</td>\n",
       "      <td>I'm disappointed with the flavor. The chocolat...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568451</th>\n",
       "      <td>B004I613EE</td>\n",
       "      <td>pksd \"pk_007\"</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1329782400</td>\n",
       "      <td>These stars are small, so you can give 10-15 o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568452</th>\n",
       "      <td>B004I613EE</td>\n",
       "      <td>Kathy A. Welch \"katwel\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1331596800</td>\n",
       "      <td>These are the BEST treats for training and rew...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568453</th>\n",
       "      <td>B001LR2CU2</td>\n",
       "      <td>srfell17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1338422400</td>\n",
       "      <td>I am very satisfied ,product is as advertised,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>568454 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ProductId                      ProfileName  HelpfulnessNumerator  \\\n",
       "0       B001E4KFG0                       delmartian                     1   \n",
       "1       B00813GRG4                           dll pa                     0   \n",
       "2       B000LQOCH0  Natalia Corres \"Natalia Corres\"                     1   \n",
       "3       B000UA0QIQ                             Karl                     3   \n",
       "4       B006K2ZZ7K    Michael D. Bigham \"M. Wassir\"                     0   \n",
       "...            ...                              ...                   ...   \n",
       "568449  B001EO7N10                 Lettie D. Carter                     0   \n",
       "568450  B003S1WTCU                        R. Sawyer                     0   \n",
       "568451  B004I613EE                    pksd \"pk_007\"                     2   \n",
       "568452  B004I613EE          Kathy A. Welch \"katwel\"                     1   \n",
       "568453  B001LR2CU2                         srfell17                     0   \n",
       "\n",
       "        HelpfulnessDenominator        Time  \\\n",
       "0                            1  1303862400   \n",
       "1                            0  1346976000   \n",
       "2                            1  1219017600   \n",
       "3                            3  1307923200   \n",
       "4                            0  1350777600   \n",
       "...                        ...         ...   \n",
       "568449                       0  1299628800   \n",
       "568450                       0  1331251200   \n",
       "568451                       2  1329782400   \n",
       "568452                       1  1331596800   \n",
       "568453                       0  1338422400   \n",
       "\n",
       "                                                     Text  Sentiment  \n",
       "0       I have bought several of the Vitality canned d...          1  \n",
       "1       Product arrived labeled as Jumbo Salted Peanut...         -1  \n",
       "2       This is a confection that has been around a fe...          0  \n",
       "3       If you are looking for the secret ingredient i...         -1  \n",
       "4       Great taffy at a great price.  There was a wid...          1  \n",
       "...                                                   ...        ...  \n",
       "568449  Great for sesame chicken..this is a good if no...          1  \n",
       "568450  I'm disappointed with the flavor. The chocolat...         -1  \n",
       "568451  These stars are small, so you can give 10-15 o...          1  \n",
       "568452  These are the BEST treats for training and rew...          1  \n",
       "568453  I am very satisfied ,product is as advertised,...          1  \n",
       "\n",
       "[568454 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f38af9a",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8eb80a2",
   "metadata": {},
   "source": [
    "We understand from the previous EDA that we need to handle clean ProfileName and add 3 new variables(user review frequency, product review frequency, spam frequency). This all will be done within the preprocessing step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed128c0",
   "metadata": {},
   "source": [
    "We first split the data into 3 parts before preprocessing them. We split them into train data, test data, and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4882e6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "xtrain, xsplit, ytrain, ysplit = train_test_split(raw.drop('Sentiment',axis = 1),\n",
    "                                                  raw.Sentiment, \n",
    "                                                  test_size = 0.3, \n",
    "                                                  random_state = 42)\n",
    "xtest, xval, ytest, yval = train_test_split(xsplit,ysplit,\n",
    "                                            test_size = 0.5,\n",
    "                                            random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "384c61ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "#install python-Levenshtein too if you haven't already did\n",
    "\n",
    "#===========================================================================#\n",
    "#                         TOKENIZATION FUNCTION                             #\n",
    "#===========================================================================#\n",
    "\n",
    "def tokenize(txt):\n",
    "    import re\n",
    "    from wordcloud import STOPWORDS\n",
    "    import nltk\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.stem.wordnet import WordNetLemmatizer\n",
    "    stpwrd = set(STOPWORDS)\n",
    "    #we then add frequent irrelevant words discovered before \n",
    "    stpwrd.update(['br','href','good', 'amazon', 'one', 'taste', 'make', 'flavor','product'])\n",
    "    text = re.sub(r\"[^a-zA-Z0-9'\\s]\",\" \", txt.lower())\n",
    "    words = word_tokenize(text)\n",
    "    lemma = [WordNetLemmatizer().lemmatize(w) for w in words if w not in stpwrd]\n",
    "    return lemma\n",
    "\n",
    "\n",
    "\n",
    "#===========================================================================#\n",
    "#                         PREPROCESS AND ENGINEER                           #\n",
    "#===========================================================================#\n",
    "\n",
    "def preprocess_engineer(data,train = True):\n",
    "    #this is to handle missing values before using the data\n",
    "    data['ProfileName'] = data['ProfileName'].apply(lambda x: 'Anonymous' if \n",
    "                                                (x == 'nan')|(x == 'NaN')|\n",
    "                                                (x == 'N/A')|(x == '0')|\n",
    "                                                (x == '')|(x == '-1')|\n",
    "                                                (x == 'null')|(x == 'Null')|\n",
    "                                                (x == 'NA')|(x == 'na')|\n",
    "                                                (x == 'none')|(x == 'unknown') else x)\n",
    "    #this is to simulate that after data train, we face a number of new reviews\n",
    "    if train == True:\n",
    "        countprof = data.ProfileName.value_counts().reset_index()\n",
    "        countprof.columns = ['ProfileName','ProfReviewCount']\n",
    "        \n",
    "        countprod = data.ProductId.value_counts().reset_index()\n",
    "        countprod.columns = ['ProductId','ProdReviewCount']\n",
    "        \n",
    "        countspam = data.groupby(['ProductId','ProfileName']).size().reset_index()\n",
    "        countspam.columns = ['ProductId','ProfileName','SpamReviewCount']\n",
    "        \n",
    "        engineered = pd.merge(\n",
    "                        pd.merge(\n",
    "                            pd.merge(data,countprof,how='left',on='ProfileName'),\n",
    "                            countprod,how = 'left', on = 'ProductId'),\n",
    "                        countspam, how = 'left',on = ['ProductId','ProfileName'])\n",
    "        return engineered      \n",
    "    if train == False:\n",
    "        #this is from train\n",
    "        countproftrain = xtrain.ProfileName.value_counts().reset_index()\n",
    "        countproftrain.columns = ['ProfileName','ProfReviewCounttrain']\n",
    "        \n",
    "        countprodtrain = xtrain.ProductId.value_counts().reset_index()\n",
    "        countprodtrain.columns = ['ProductId','ProdReviewCounttrain']\n",
    "        \n",
    "        countspamtrain = xtrain.groupby(['ProductId','ProfileName']).size().reset_index()\n",
    "        countspamtrain.columns = ['ProductId','ProfileName','SpamReviewCounttrain']\n",
    "        \n",
    "        \n",
    "        \n",
    "        #this is from the newly introduced data\n",
    "        countprof = data.ProfileName.value_counts().reset_index()\n",
    "        countprof.columns = ['ProfileName','ProfReviewCountadd']\n",
    "        \n",
    "        countprod = data.ProductId.value_counts().reset_index()\n",
    "        countprod.columns = ['ProductId','ProdReviewCountadd']\n",
    "        \n",
    "        countspam = data.groupby(['ProductId','ProfileName']).size().reset_index()\n",
    "        countspam.columns = ['ProductId','ProfileName','SpamReviewCountadd']\n",
    "        \n",
    "        \n",
    "        \n",
    "        #this is to add newly introduced data with train\n",
    "        #ProfileName\n",
    "        countproffinal = countprof.merge(countproftrain,how = 'left',on = 'ProfileName')\n",
    "        countproffinal.fillna(0,inplace = True)\n",
    "        countproffinal['ProfReviewCount'] = countproffinal.ProfReviewCounttrain + countproffinal.ProfReviewCountadd\n",
    "        countproffinal.drop(['ProfReviewCounttrain','ProfReviewCountadd'],axis = 1,inplace = True)\n",
    "    \n",
    "        #ProductId\n",
    "        countprodfinal = countprod.merge(countprodtrain,how = 'left',on = 'ProductId')\n",
    "        countprodfinal.fillna(0,inplace = True)\n",
    "        countprodfinal['ProdReviewCount'] = countprodfinal.ProdReviewCounttrain + countprodfinal.ProdReviewCountadd\n",
    "        countprodfinal.drop(['ProdReviewCounttrain','ProdReviewCountadd'],axis = 1,inplace = True)\n",
    "        \n",
    "        #SpamReviewCount\n",
    "        countspamfinal = countspam.merge(countspamtrain,how = 'left',on = ['ProductId','ProfileName'])\n",
    "        countspamfinal.fillna(0,inplace = True)\n",
    "        countspamfinal['SpamReviewCount'] = countspamfinal.SpamReviewCounttrain + countspamfinal.SpamReviewCountadd\n",
    "        countspamfinal.drop(['SpamReviewCounttrain','SpamReviewCountadd'],axis = 1,inplace = True)\n",
    "        \n",
    "        engineered = pd.merge(\n",
    "                        pd.merge(\n",
    "                            pd.merge(data,countproffinal,how='left',on='ProfileName'),\n",
    "                            countprodfinal,how = 'left', on = 'ProductId'),\n",
    "                        countspamfinal, how = 'left',on = ['ProductId','ProfileName'])\n",
    "        return engineered  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d09ae04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\921000~1\\AppData\\Local\\Temp/ipykernel_10252/3377171446.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['ProfileName'] = data['ProfileName'].apply(lambda x: 'Anonymous' if\n"
     ]
    }
   ],
   "source": [
    "xtrainpreprocessed = preprocess_engineer(xtrain,train = True)\n",
    "xtestpreprocessed = preprocess_engineer(xtest,train = False)\n",
    "xvalpreprocessed = preprocess_engineer(xval,train = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "498b7904",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a custom Word2Vec model\n",
    "class CustomW2V(object):\n",
    "    def __init__(self,word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        #if a text is empty we should return a vector of zeros\n",
    "        #with the same dimensionality as all the other vectors\n",
    "        self.dim = len(next(iter(word2vec.values())))\n",
    "    def fit(self,x):\n",
    "        return self\n",
    "    def transform(self,x):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                   or [np.zeros(self.dim)],axis = 0)\n",
    "            for words in x\n",
    "        ])\n",
    "\n",
    "model = Word2Vec(xtrainpreprocessed.Text.apply(tokenize),\n",
    "                 sg = 0,\n",
    "                 hs = 1,\n",
    "                 seed = 42,\n",
    "                 vector_size = 128)\n",
    "w2v = dict(zip(model.wv.index_to_key, model.wv.vectors))\n",
    "modelw = CustomW2V(w2v)\n",
    "\n",
    "#convert to text \n",
    "xtrainvect = modelw.transform(xtrainpreprocessed.Text.apply(tokenize))\n",
    "xtestvect = modelw.transform(xtestpreprocessed.Text.apply(tokenize))\n",
    "xvalvect = modelw.transform(xvalpreprocessed.Text.apply(tokenize))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54658f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the remaining features\n",
    "def vecttodata(aftervec,beforevec):\n",
    "    df = pd.DataFrame(aftervec)\n",
    "    df['HelpfulnessNumerator'] = beforevec.reset_index(drop=True)['HelpfulnessNumerator']\n",
    "    df['HelpfulnessDenominator'] = beforevec.reset_index(drop=True)['HelpfulnessNumerator']\n",
    "    df['Time'] = beforevec.reset_index(drop=True)['Time']\n",
    "    df['ProfReviewCount'] = beforevec.reset_index(drop=True)['ProfReviewCount']\n",
    "    df['ProdReviewCount'] = beforevec.reset_index(drop=True)['ProdReviewCount']\n",
    "    df['SpamReviewCount'] = beforevec.reset_index(drop=True)['SpamReviewCount'] \n",
    "    \n",
    "    return df\n",
    "    \n",
    "xtraindf = vecttodata(xtrainvect,xtrainpreprocessed)\n",
    "xtestdf = vecttodata(xtestvect,xtestpreprocessed)\n",
    "xvaldf = vecttodata(xvalvect,xvalpreprocessed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc2153b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "sample_weights = compute_sample_weight(\n",
    "    class_weight='balanced',\n",
    "    y=ytrain\n",
    ")\n",
    "\n",
    "logreg = LogisticRegression(random_state = 42)\n",
    "logreg.fit(scaler.fit_transform(xtraindf),ytrain,sample_weight = sample_weights)\n",
    "pred = logreg.predict(scaler.transform(xtestdf))\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "print(classification_report(pred,ytest, target_names = ['Negative','Neutral','Positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d08bb6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.63      0.67      0.65     17535\n",
      "     Neutral       0.84      0.17      0.28     60099\n",
      "    Positive       0.13      0.95      0.23      7635\n",
      "\n",
      "    accuracy                           0.34     85269\n",
      "   macro avg       0.53      0.60      0.39     85269\n",
      "weighted avg       0.73      0.34      0.35     85269\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#now to test against purely another unseen data\n",
    "\n",
    "finalpred = logreg.predict(scaler.transform(xvaldf))\n",
    "print(classification_report(finalpred,yval, target_names = ['Negative','Neutral','Positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cbeed1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    0.638322\n",
       "-1    0.220018\n",
       " 0    0.141660\n",
       "Name: Sentiment, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrain.value_counts()/len(ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c2550af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('titanium', 0.4521552324295044),\n",
       " ('silicon', 0.4399755299091339),\n",
       " ('tnr', 0.4061760902404785),\n",
       " ('clasico', 0.36855122447013855),\n",
       " ('cornmeal', 0.36003872752189636),\n",
       " ('vine', 0.3571547567844391),\n",
       " ('btwn', 0.35447293519973755),\n",
       " ('sulfur', 0.35143882036209106),\n",
       " ('sulphur', 0.35117125511169434),\n",
       " ('bread', 0.3475218713283539)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive = ['good'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb100e7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7885308"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('great','awesome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5d74f25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7885309"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cosine similarity manual\n",
    "v1 = model.wv['great']\n",
    "v2 = model.wv['awesome']\n",
    "np.dot(v1, v2)/(np.linalg.norm(v1)*np.linalg.norm(v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b01f9d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.78950328],\n",
       "       [0.78950328, 1.        ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(v1,v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1ea936",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
